<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <link href='https://fonts.googleapis.com/css?family=Chivo:900' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen" />
    <link rel="stylesheet" type="text/css" href="stylesheets/pygment_trac.css" media="screen" />
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print" />
    <script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <title>Clustering</title>
  </head>

  <body>
    <div id="container">
      <div class="inner">

        <header>
          <h1>Parallel Clustering</h1>
          <h2>Carnegie Mellon, 15-456</h2>
        </header>

        <section id="downloads" class="clearfix">
          <a href="https://github.com/laxmandhulipala/ComputationalGeometry/zipball/master" id="download-zip" class="button"><span>Download .zip</span></a>
          <a href="https://github.com/laxmandhulipala/ComputationalGeometry/tarball/master" id="download-tar-gz" class="button"><span>Download .tar.gz</span></a>
          <a href="https://github.com/laxmandhulipala/ComputationalGeometry" id="view-on-github" class="button"><span>View on GitHub</span></a>
        </section>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>

        <hr>

        <section id="main_content">
          <h2>Clustering</h2>

<p>
Very broadly, the Clustering problem is to place place objects, for our purposes points in \(\mathbb{R}^{n}\),
in groups such that the within-group similarity is very high. Concretely, given a set of objects \(P\), and a set 
of centers \(C\) in some metric
space \(\mathcal{X}, d\), where \(d : \mathcal{X} \times \mathcal{X} \mapsto [0, \infty) \) satisfying the standard 
axioms of a metric, we can assign every point of \(P\) to it's nearest neighbour in \(C\). Typically, there is also a 
cost associated with a clustering, which we minimize. 
</p>

<h3>K-Means Clustering</h3>
<p>
Suppose we are given a set of points, \(P, |P| = n\), as well as a parameter \(k \in \mathbb{N}\). The k-means problem
is to find a set of \(k\) points which minimize the sum-of-squares distances of every point in $P$ to their closest
point in $C$. Concretely, this cost is
$$ |P_{c}|^{2}_{2} = \sum_{p \in P} d_{\mathcal{M}}(p,C))^{2} $$
where the $k$-means problem is to then find a set $k$ centers, $C$, such that 
$\|P_{c}\|^{2}_{2}$ is minimized.
</p>
<h3>Sequential K-Means</h3>
<p>
The simplest, and perhaps most intuitive algorithm for computing a set of $C$ centers is Lloyd's Algorithm, 
which is done iteratively by relaxing the Voronoi diagram at each step. The algorithm proceed as follows : 
<pre><code>proc kMeans(P,k)
  C :=  k initial centers randomly chosen from P.
  convergence := +infty
  repeat while (convergence > epsilon):
    drift := 0
    for all p in P:
      assign p to the cluster of it's nearest neighbour
    for all c in C:
      nC := centroid of c's cluster
      drift := max(drift, d(nC,c))
      c := nC
    convergence := drift
  return C
</code></pre>
</p>
<p>
The centroid is the mean of the points in a cluster. As we iteratively set new cluster centers to be the
centroids of their cluster, points which were initially not a part of a cluster could then join the cluster. 
We terminate the algorithm when the convergence, which captures how far the centroids moved during the last
iteration is less than some $\epsilon$. 
</p>

<h3>Parallel K-Means</h3>
<p>
  We can optimize the k-means algoritm for cluster-based computing settings by dividing splitting $P$, the input
  point set amongst a set of $p \geq 2$ processors. The work on a node then 
  becomes to centroid-sums for each of the $k$ current, global centers. The node then sends its local 
  centroid-sums back to a master-node, who calculates a global sum for each centroid, and divides this sum by the number of 
  points assigned to that center, globally, to calculate the new center. Dividing up the pseudo-code into the master task
  and slave task, we have

<pre class="brush: python"></code> Master:
  C := k initial centers randomly chosen from P
  convergence := +infty
  split P up amongst all p nodes
  repeat while (convergence > epsilon):
    drift := 0  
    nodeCents := Gather(localCentroidSums,(p1,p2,...,pn))
    for all c in C:
     
Slave:
  localP := subset of P recieved from Master
  convergence := +infty
  repeat while (convergence > epsilon):
    C := Receive(global centers from Master)
    for all p in P:
      assign p to the cluster of it's nearest neighbour
    localCentroidSums := <>
    for all c in C:
      localSum.sum := sum of all p in c's cluster
      localsum.numPts := size of c's cluster
      localCentroidSums += localSum
    Send(localCentroidSum to Master)
    
</code></pre>
</p>


<p>
  Because we wait until this single-node join to calculate the new centroid, it's clear that this algorithm is
  equivalent to the sequential algorithm, in that it converges in the same number of iterations. We make our gains
  in the fact that any given node is only dealing with a $\frac{|P|}{p}$ sized set of points. However, we do pay 
  a communication overhead
</p>



<h3>Coresets</h3>




<h3>Coresets</h3>
A z

        </section>

        <footer>
          This site is maintained by <a href="https://github.com/laxmandhulipala">laxmandhulipala</a><br>
        </footer>

        
      </div>
    </div>
  </body>
</html>
